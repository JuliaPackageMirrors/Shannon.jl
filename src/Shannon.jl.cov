        - module Shannon
        - 
        - # The entropy functions (ChaoShen, Dirichlet, MillerMadow) were copied from the
        - # R package "entropy", by Jean Hausser and Korbinian Strimmer. For details, see
        - # http://cran.r-project.org/web/packages/entropy/index.html
        - 
        - using StatsBase
        - 
        - export KL, PI, MI, entropy
        - export bin_vector, bin_matrix, bin_value
        - export unbin_value, unbin_matrix, unbin_vector 
        - export combine_binned_matrix, combine_binned_vector
        - export combine_and_relabel_binned_matrix
        - export unary_of_matrix
        - export relabel
        - 
        - function KL(p::Vector{Float64}, q::Vector{Float64}; base=2)
        -   @assert (length(p) == length(q)) "Size mismatch"
        -   sum([ (p[i] != 0 && q[i] != 0)? p[i] * log(base, p[i]/q[i]) : 0 for i=1:length(p)])
        - end
        - 
        - # predictive information
        - PI(data::Vector{Int64}; base=2, mode="emperical") = MI(hcat(data[1:end-1], data[2:end]), base=base, mode=mode)
        - 
        - # mutual information
        - function MI(data::Matrix{Int64}; base=2, mode="emperical", pseudocount=0)
        - 
        -   max = maximum(data)
        -   px  = fe1p(data[:,1])
        -   py  = fe1p(data[:,2])
        -   pxy = fe2p(hcat(data[:,[1:2]]))
        - 
        -   r = 0
        -   for x=1:length(px)
        -     for y=1:length(py)
        -       if px[x] != 0.0 && py[y] != 0.0 && pxy[x,y] > 0.0
        -         r = r + pxy[x,y] * (log(base, pxy[x,y]) - log(base, px[x]*py[y]))
        -       end
        -     end
        -   end
        -   r
        - end
        - 
        6 entropy_emperical(p::Vector{Float64}, base::Number) = sum([ p[x] > 0 ? (-p[x] * log(base, p[x])) : 0 for x=1:size(p)[1]])
        - 
        - function entropy_chaoshen(data::Vector{Int64}, base::Number)
        4   c = counts(data, 1:maximum(data))
        4   n = sum(c)
        4   p = c ./ n
        4   s = sum(p)
        4   p = p ./ s # to be sure
        - 
        -   # code copied form R entropy package
        4   f1 = sum([i == 1? 1 : 0 for i in c]) # number of singletons
        4   if (f1 == n)
        4     f1 = n-1                # avoid C=0
        -   end
        - 
        4   C  = 1 - f1/n              # estimated coverage
        4   pa = C .* p                # coverage adjusted empirical frequencies
        4   la = (1-(1-pa).^n)         # probability to see a bin (species) in the sample
        -   #= -sum(pa.*log(base, pa)./la) # Chao-Shen (2003) entropy estimator =#
        4   r = 0
        4   for i=1:length(pa)
       24     if pa[i] != 0
       24       r -= pa[i] * log(base, pa[i]) / la[i]
        -     end
        -   end
        4   r
        - end
        - 
        - function entropy_millermadow(data::Vector{Int64}, base::Number)
        1   c = counts(data, 1:maximum(data))
        1   n = sum(c)                         # total number of counts
        1   m = sum([i > 0? 1 : 0 for i in c]) # number of bins with non-zero counts
        1   p = c ./ n
        1   s = sum(p)
        1   p = p ./ s
        - 
        -   # bias-corrected empirical estimate
        1   entropy_emperical(p, base) + (m-1)/(2*n)
        - end
        - 
        - function entropy(data::Vector{Int64}; base=2, mode="emperical", pseudocount=0)
       10   known_mode = (mode == "emperical" ||
        -                 mode == "ChaoShen"  ||
        -                 mode == "Dirichlet" ||
        -                 mode == "MillerMadow")
       10   @assert known_mode "Mode may be any of the following: [\"emperical\", \"ChaoShen\", \"Dirichlet\", \"MillerMadow\"]"
        - 
       10   p = []
        - 
       10   r = nothing
        - 
       10   if     mode == "emperical"
        3     p = fe1p(data)
        3     r = entropy_emperical(p, base) 
        7   elseif mode == "ChaoShen"
        4     r = entropy_chaoshen(data, base)
        3   elseif mode == "Dirichlet"
        2     p = fe1pd(data, pseudocount)
        2     r = entropy_emperical(p, base)
        1   elseif mode == "MillerMadow"
        1     r = entropy_millermadow(data, base)
        -   #= else =# # Not needed. Caught by assertion
        -   end
       10   r
        - end
        - 
      163 bin_value(v::Float64, bins::Int64, min=-1.0, max=1.0)                     = int64(maximum([maximum([minimum([1.0, (v-min) / (max - min)]), 0.0]) * bins, 1.0]))
      140 bin_vector(vec::Vector{Float64}, min::Float64, max::Float64, bins::Int64) = map(v->bin_value(v, bins, min, max), vec)
        - 
        - unbin_vector(vec::Vector{Float64}, min::Float64, max::Float64, bins::Int64; mode="centre") = map(v->unbin_value(v, bins, min, max, mode=mode), vec)
        - 
        - function unbin_value(v::Int64, bins::Int64, min=-1.0, max=1.0; mode="centre")
       30   known_mode = (mode == "centre" ||
        -                 mode == "lower"  ||
        -                 mode == "upper")
       30   @assert known_mode "Mode may be any of the following: [\"centre\", \"lower\", \"upper\"]"
        - 
       30   delta = (max - min) / float64(bins)
       30   u = (v - 1) * delta + min
        - 
       30   if     mode == "centre"
       10     return u + 0.5 * delta
       20   elseif mode == "upper"
       10     return u + delta
        -   end
       10   return u
        - end
        - 
        - function bin_matrix(m::Matrix{Float64}, min::Float64, max::Float64, bins::Int64)
        3   r = zeros(size(m))
        3   for i=1:size(m)[1]
       16     r[i,:] = bin_vector(squeeze(m[i,:],1), min, max, bins)
        -   end
        3   convert(Matrix{Int64}, r)
        - end
        - 
        - function unbin_matrix(m::Matrix{Float64}, min::Float64, max::Float64, bins::Int64; mode="centre")
        -   r = zeros(size(m))
        -   for i=1:size(m)[1]
        -     r[i,:] = unbin_vector(squeeze(m[i,:],1), min, max, bins, mode=mode)
        -   end
        -   convert(Matrix{Int64}, r)
        - end
        - 
        - function combine_binned_vector(v::Vector{Int64}, bins::Int64)
       12   r = 0
       12   for i = 1:length(v)
       48     r += v[i] * bins^(i-1)
        -   end
       12   convert(Int64, r)
        - end
        - 
        - function combine_binned_matrix(v::Matrix{Int64})
        1   l = size(v)[1]
        1   r = zeros(l)
        1   m = maximum(v)
        1   for i = 1:l
        4     r[i] = combine_binned_vector(squeeze(v[i,:],1), m)
        -   end
        1   convert(Vector{Int64}, r)
        - end
        - 
        - relabel(v::Vector{Int64}) = indexin(v, unique(v))
        - 
        - combine_and_relabel_binned_matrix(data::Matrix{Int64}) = relabel(combine_binned_matrix(data))
        - 
        - unary_of_matrix(data::Matrix{Float64}, min::Float64, max::Float64, bins::Int64) = combine_and_relabel_binned_matrix(bin_matrix(data, min, max, bins))
        - 
        - fe1ph(v::Vector{Int64})   = hist(v)[2] ./ size(v)[1]
        - 
        - function fe2p(v::Matrix{Int64}) # frequency estimation of one dimensional probability
        -   m1 = maximum(v[:,1])
        -   m2 = maximum(v[:,2])
        -   r  = counts(v[:,1], v[:,2], (1:m1, 1:m2))
        -   l  = size(v)[1]
        -   r = r ./ l
        -   # just to get rid of the numerical inaccuracies and make sure its a probability distribution
        -   s = sum(r)
        -   r ./ s
        - end
        - 
        - # frequency estimation of one dimensional probability
        - function fe1p(v::Vector{Int64})
        3   m = maximum(v)
        3   l  = size(v)[1]
        3   r  = counts(v, 1:m)
        3   r = r ./ l
        -   # just to get rid of the numerical inaccuracies and make sure its a probability distribution
        3   s = sum(r)
        3   r ./ s
        - end
        - 
        - function fe1pd(v::Vector{Int64}, d::Number) # Dirichlet
        2   r = counts(v, 1:maximum(v))
        2   r = r .+ d
        2   l = sum(r)
        2   r = r ./ l
        2   s = sum(r) # just to be sure
        2   r ./ s
        - end
        - 
        - end # module
        - 
